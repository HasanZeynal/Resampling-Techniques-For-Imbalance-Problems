{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasanzeynal/Resampling-Techniques-For-Imbalance-Problems/blob/main/credit_fraud_detection/01-credit_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fs4luK1rR20"
      },
      "source": [
        "# Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugPj6j2Xc3Gi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7qVQ50OLO6g"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/datasets/creditcard.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfiCfPhI0Jn"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjXoiFVfGUPR"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qppsR-1EFxQw"
      },
      "source": [
        "## Checking Duplicated Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RrGNCn71puO"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQsmNqrN1yjT"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNWFBq5K2PeG"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAsbXyZsGEf-"
      },
      "source": [
        "## Checking for Null Values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-am8utwJBxN"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "  if df[col].isnull().sum() != 0:\n",
        "    print(f'{col}:{df[col].isnull().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSxuhqlSRnqF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "null_coll = ['V21','V22','V23','V24','V25',\n",
        "             'V26','V27','V28','Amount']\n",
        "for col in null_coll:\n",
        "  df[null_coll] = df[null_coll].replace(np.nan,df[null_coll].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy_MAv0uTSYu"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of features that has null values {df.isnull().any().sum()}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4BoVYz3w5Ka"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "df['Class'] = df['Class'].replace({np.nan:1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9BpXa8tsP8f"
      },
      "source": [
        "# Data Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6aj3RC9I2Aj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cheking to see imbalance problem"
      ],
      "metadata": {
        "id": "BZL6HP2GFhVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLIIFv5eI7jy"
      },
      "outputs": [],
      "source": [
        "colors = ['#FFD700','#3B3B3C']\n",
        "fraud = len(df[df['Class'] == 1]) / len(df) * 100\n",
        "nofraud = len(df[df['Class'] == 0]) / len(df) * 100\n",
        "fraud_percentage = [nofraud,fraud]\n",
        "\n",
        "fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.pie(fraud_percentage,labels = ['Fraud','No Fraud'],autopct='%1.1f%%',startangle = 90,colors = colors,\n",
        "       wedgeprops = {'edgecolor' : 'black','linewidth': 2,'antialiased' : True})\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.countplot(x = 'Class',data = df,edgecolor = 'black',palette = colors)\n",
        "for rect in ax.patches:\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n",
        "ax.set_xticklabels(['No Fraud','Fraud'])\n",
        "plt.title('Number of Fraud Cases');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation"
      ],
      "metadata": {
        "id": "mUGZtqeWFrZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpbrRYe90Eux"
      },
      "outputs": [],
      "source": [
        "corr = df.corr()\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cheking for normalization in the each column"
      ],
      "metadata": {
        "id": "Nm2k0vYWFvN3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ztImwRt9fEC"
      },
      "outputs": [],
      "source": [
        "skewed_cols = []\n",
        "for cols in df.columns:\n",
        "  if df[cols].dtype != 'object':\n",
        "    if np.abs(df[cols].skew()) > 1:\n",
        "      skewed_cols.append(cols)\n",
        "skewed_cols.remove('Class')\n",
        "print(skewed_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuIRh7bBH5B"
      },
      "outputs": [],
      "source": [
        "colors = ['#B3F9C5', '#f9c5b3']\n",
        "#first row\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V1'],ax=ax1,color = 'purple')\n",
        "ax1.set_title('V1')\n",
        "sns.distplot(df['V2'],ax=ax2,color = 'magenta')\n",
        "ax2.set_title('V2')\n",
        "#second\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V3'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V3')\n",
        "sns.distplot(df['V5'],ax=ax2,color='purple')\n",
        "ax2.set_title('V5')\n",
        "#third\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V6'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V6')\n",
        "sns.distplot(df['V7'],ax=ax2,color='purple')\n",
        "ax2.set_title('V7')\n",
        "#fourth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V8'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V8')\n",
        "sns.distplot(df['V10'],ax=ax2,color='purple')\n",
        "ax2.set_title('V10')\n",
        "#fifth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V12'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V12')\n",
        "sns.distplot(df['V14'],ax=ax2,color='purple')\n",
        "ax2.set_title('V14')\n",
        "#sixth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V16'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V16')\n",
        "sns.distplot(df['V17'],ax=ax2,color='purple')\n",
        "ax2.set_title('V17')\n",
        "#seventh\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V20'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V20')\n",
        "sns.distplot(df['V21'],ax=ax2,color='purple')\n",
        "ax2.set_title('V21')\n",
        "#eight\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V23'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V23')\n",
        "sns.distplot(df['V28'],ax=ax2,color='purple')\n",
        "ax2.set_title('V28')\n",
        "#nineth\n",
        "sns.distplot(df['Amount'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('Amount')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OedWCmgPUeM5"
      },
      "outputs": [],
      "source": [
        "print(f\"Skew:{df.Amount.skew()}\")\n",
        "print(f\"Max:{df.Amount.max()}\\nMin:{df.Amount.min()}\")\n",
        "print(f\"Std:{df.Amount.std()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrVkarZdORMe"
      },
      "outputs": [],
      "source": [
        "corr_of_df = pd.DataFrame(df.corr())\n",
        "print(f\"The corr of time by class is {corr_of_df['Class'][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnkEx8YcWPMT"
      },
      "outputs": [],
      "source": [
        "df.drop('Time',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umvuBGTnvPtl"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "  print(f'{col}:{df[col].min()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ve4F43_veRn"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEViJfEoJBcy"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "  print(f'{col}:{df[col].skew()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTwrx3P7FlOt"
      },
      "outputs": [],
      "source": [
        "mean_skew = list()\n",
        "for col in df.columns:\n",
        "  if col == 'Class':\n",
        "    continue\n",
        "  mean_skew.append(np.abs(df[col].skew()))\n",
        "print(f'The mean of skewness:{(sum(mean_skew)/len(mean_skew))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation"
      ],
      "metadata": {
        "id": "XgXPYhByPaW_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYKxERo-wYhW"
      },
      "source": [
        "## Yoe-Johnson and Log Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEoBXrazX0iL"
      },
      "outputs": [],
      "source": [
        "df['normalized_amount'] = np.log10(df['Amount'] + 0.01) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdyKFNoiYprt"
      },
      "outputs": [],
      "source": [
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['Amount'],ax=ax1)\n",
        "#plt.text('high skewness')\n",
        "ax1.set_title('Amount Distribution before Normalization')\n",
        "sns.distplot(df['normalized_amount'],ax=ax2)\n",
        "#plt.text('high skewness')\n",
        "ax2.set_title('Amount Distribution after Log Normalization')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_siHBFWoRCgl"
      },
      "outputs": [],
      "source": [
        "df.drop('Amount',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5M0vsqUMraL"
      },
      "outputs": [],
      "source": [
        "skewed_cols.remove('Amount')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSKloRd4x21m"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import yeojohnson\n",
        "for col in skewed_cols:\n",
        "    df[col],lam = yeojohnson(df[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbdRaIUs_3CP"
      },
      "source": [
        "## Finding how skewness changed after normalization methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnrH1VIb_y5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b72b814-151f-4d17-df50-0852ddf413d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean of skewness:0.68\n"
          ]
        }
      ],
      "source": [
        "mean_skew = list()\n",
        "for col in df.columns:\n",
        "  if col == 'Class':\n",
        "    continue\n",
        "  mean_skew.append(np.abs(df[col].skew()))\n",
        "print(f'The mean of skewness:{round(sum(mean_skew)/len(mean_skew),2)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsSL_bR8zdWT"
      },
      "outputs": [],
      "source": [
        "colors = ['#B3F9C5', '#f9c5b3']\n",
        "#first row\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V1'],ax=ax1,color = 'purple')\n",
        "ax1.set_title('V1')\n",
        "sns.distplot(df['V2'],ax=ax2,color = 'magenta')\n",
        "ax2.set_title('V2')\n",
        "#second\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V3'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V3')\n",
        "sns.distplot(df['V5'],ax=ax2,color='purple')\n",
        "ax2.set_title('V5')\n",
        "#third\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V6'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V6')\n",
        "sns.distplot(df['V7'],ax=ax2,color='purple')\n",
        "ax2.set_title('V7')\n",
        "#fourth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V8'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V8')\n",
        "sns.distplot(df['V10'],ax=ax2,color='purple')\n",
        "ax2.set_title('V10')\n",
        "#fifth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V12'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V12')\n",
        "sns.distplot(df['V14'],ax=ax2,color='purple')\n",
        "ax2.set_title('V14')\n",
        "#sixth\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V16'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V16')\n",
        "sns.distplot(df['V17'],ax=ax2,color='purple')\n",
        "ax2.set_title('V17')\n",
        "#seventh\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V20'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V20')\n",
        "sns.distplot(df['V21'],ax=ax2,color='purple')\n",
        "ax2.set_title('V21')\n",
        "#eight\n",
        "f,(ax1,ax2) = plt.subplots(1,2,figsize=(16,9))\n",
        "sns.distplot(df['V23'],ax=ax1,color = 'blue')\n",
        "ax1.set_title('V23')\n",
        "sns.distplot(df['V28'],ax=ax2,color='purple')\n",
        "ax2.set_title('V28')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tOTFX3E4maT"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD0fl5WGonsj"
      },
      "outputs": [],
      "source": [
        "skewed_cols = []\n",
        "for cols in df.columns:\n",
        "  if df[cols].dtype != 'object':\n",
        "    if np.abs(df[cols].skew()) > 1:\n",
        "      skewed_cols.append(cols)\n",
        "print(skewed_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling with Keras "
      ],
      "metadata": {
        "id": "SKmM-5W9WMvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1fp7wW5RCjW"
      },
      "outputs": [],
      "source": [
        "y = df.iloc[:,-2]\n",
        "X = df.drop('Class',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwtE96iPQiT3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42,test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SYxsxUtQoP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943f4045-856e-420d-c33f-2160316dc8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(198608, 29)\n",
            "(85118, 29)\n",
            "(198608,)\n",
            "(85118,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1XdW95UVHmf"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Small Model"
      ],
      "metadata": {
        "id": "jqHIW4kuWcPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4-EgA76Vbfo"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7Ih9dgPVqoK"
      },
      "outputs": [],
      "source": [
        "small_model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIsmF-eCbvGC"
      },
      "outputs": [],
      "source": [
        "small_model.add(Dense(units = 32,activation='relu',input_shape=(29,)))\n",
        "small_model.add(Dense(units = 64,activation = 'relu'))\n",
        "small_model.add(Dense(units = 1,activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c15eo7vnaEbV"
      },
      "outputs": [],
      "source": [
        "small_model_history = small_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001 ),\n",
        "                    metrics = ['accuracy'],\n",
        "                    loss = tf.keras.losses.BinaryCrossentropy())\n",
        "\n",
        "small_model_history = small_model.fit(X_train,y_train,\n",
        "                batch_size = 64,\n",
        "                epochs = 10\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7433Qq648nr"
      },
      "outputs": [],
      "source": [
        "test_loss,test_accuracy = small_model.evaluate(X_test,y_test)\n",
        "train_loss,train_accuracy = small_model.evaluate(X_train,y_train)\n",
        "print(f\"Test accuracy:{round(test_accuracy * 100,2)}\")\n",
        "print(f\"Train accuracy:{round(train_accuracy * 100,2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doFzdXXW6Lfn"
      },
      "outputs": [],
      "source": [
        "small_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enplaJekB5nf"
      },
      "outputs": [],
      "source": [
        "predictions = small_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mc7Flneo78N"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to convert predictions to the 0 and 1's we din't need any probability|"
      ],
      "metadata": {
        "id": "1Q6LoF8PYvdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_yMXGAQf1m2"
      },
      "outputs": [],
      "source": [
        "np.unique(np.round(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqu1yPufCUlR"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.multiclass import type_of_target\n",
        "type_of_target(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvj_1y57Ck3d"
      },
      "outputs": [],
      "source": [
        "type_of_target(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can't any predict in the evaluation metrics with 'binary' and 'continuous' dtypes \n",
        "#that is why i converted probabilties to the discrete values"
      ],
      "metadata": {
        "id": "iu7-4GlQY9VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_mitutikbL3"
      },
      "outputs": [],
      "source": [
        "print(y_test.shape)\n",
        "print(predictions.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SFzGszLkpEy"
      },
      "outputs": [],
      "source": [
        "print(predictions.shape)\n",
        "print(predictions.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ibKh3X7pZm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
        "#fig,ax = plt.subplots(figsize=(16,9))\n",
        "confusion_matrix(y_test,np.round(predictions))\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9H8IiiM48cP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,np.round(predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Improved Model"
      ],
      "metadata": {
        "id": "j7Hbh6e8Zqec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I use Dropout Layer,Regularizations Method,Incrase the number of Hidden layers and Callback method"
      ],
      "metadata": {
        "id": "imGAdmNAaJ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c3S5p6DKgsX"
      },
      "outputs": [],
      "source": [
        "from keras.backend import dropout\n",
        "from keras import regularizers\n",
        "\n",
        "improved_model = Sequential()\n",
        "\n",
        "improved_model.add(Dense(units = 16,kernel_regularizer = regularizers.l2(0.001),activation='relu',input_shape=(29,)))\n",
        "improved_model.add(Dropout(0.5))\n",
        "improved_model.add(Dense(units = 32,kernel_regularizer= regularizers.l2(0.001),activation = 'relu'))\n",
        "improved_model.add(Dropout(0.5))\n",
        "improved_model.add(Dense(units = 1,activation = 'sigmoid'))\n",
        "\n",
        "improved_model_history = improved_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01 ),\n",
        "                    metrics = ['accuracy'],\n",
        "                    loss = tf.keras.losses.BinaryCrossentropy())\n",
        "improved_model_history = small_model.fit(X_train,y_train,\n",
        "                batch_size = 256,\n",
        "                epochs = 10,\n",
        "                validation_split = 0.2,\n",
        "                callbacks=EarlyStopping(patience=3)\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2yFoe9RKg0K"
      },
      "outputs": [],
      "source": [
        "test_loss,test_accuracy = improved_model.evaluate(X_test,y_test)\n",
        "train_loss,train_accuracy = improved_model.evaluate(X_train,y_train)\n",
        "print(f\"Test accuracy:{round(test_accuracy * 100,2)}\")\n",
        "print(f\"Train accuracy:{round(train_accuracy * 100,2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5EwS98bKg2w"
      },
      "outputs": [],
      "source": [
        "improved_model_predictions = improved_model.predict(X_test)\n",
        "confusion_matrix(y_test,np.round(improved_model_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfnC_fx_qpAO"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,np.round(improved_model_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTivKsn6VA1w"
      },
      "source": [
        "## Oversample model with duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKCmTpHRKg5X"
      },
      "outputs": [],
      "source": [
        "# class count\n",
        "class_count_0 = df['Class'].value_counts().values[0]\n",
        "class_count_1 = df['Class'].value_counts().values[1]\n",
        "\n",
        "# Separate class\n",
        "class_0 = df[df['Class'] == 0]\n",
        "class_1 = df[df['Class'] == 1]\n",
        "print('class 0:', class_0.shape)\n",
        "print('class 1:', class_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O80MgrvhKg8e"
      },
      "outputs": [],
      "source": [
        "class_1_over = class_1.sample(class_count_0, replace=True)\n",
        "\n",
        "test_over = pd.concat([class_1_over, class_0], axis=0)\n",
        "print(\"total class of 1 and 0:\\n\",test_over['Class'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(16,9))\n",
        "test_over['Class'].value_counts().plot(kind='bar', title='count (target)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU94xDhyKg_F"
      },
      "outputs": [],
      "source": [
        "test_over.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKf4xOeUfHYJ"
      },
      "outputs": [],
      "source": [
        "X = test_over.drop('Class',axis=1)\n",
        "y = test_over.iloc[:,-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGyNbmNyfHe0"
      },
      "outputs": [],
      "source": [
        "X_over_train,X_over_test,y_over_train,y_over_test = train_test_split(X,y,random_state=101,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djxWpjLbZjhh"
      },
      "outputs": [],
      "source": [
        "over_model = Sequential()\n",
        "\n",
        "over_model.add(Dense(units = 16,kernel_regularizer = regularizers.l2(0.001),activation='relu',input_shape=(29,)))\n",
        "over_model.add(Dropout(0.5))\n",
        "over_model.add(Dense(units = 32,kernel_regularizer= regularizers.l2(0.001),activation = 'relu'))\n",
        "over_model.add(Dropout(0.5))\n",
        "over_model.add(Dense(units = 1,activation = 'sigmoid'))\n",
        "\n",
        "over_model_history = over_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01 ),\n",
        "                    metrics = ['accuracy'],\n",
        "                    loss = tf.keras.losses.BinaryCrossentropy())\n",
        "over_model_history = over_model.fit(X_over_train,y_over_train,\n",
        "                batch_size = 64,\n",
        "                epochs = 10,\n",
        "                validation_split = 0.2,\n",
        "                #callbacks=EarlyStopping(patience=3)\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss,test_accuracy = improved_model.evaluate(X_test,y_test)\n",
        "train_loss,train_accuracy = improved_model.evaluate(X_train,y_train)\n",
        "print(f\"Test accuracy:{round(test_accuracy * 100,2)}\")\n",
        "print(f\"Train accuracy:{round(train_accuracy * 100,2)}\")"
      ],
      "metadata": {
        "id": "3GVC64E_kMJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have so many duplicate values that is why our model isn't good"
      ],
      "metadata": {
        "id": "0y5m80lYlSAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_model_predictions = improved_model.predict(X_test)\n",
        "confusion_matrix(y_test,np.round(improved_model_predictions))"
      ],
      "metadata": {
        "id": "2wZL9vYzkNNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,np.round(improved_model_predictions)))"
      ],
      "metadata": {
        "id": "ZUsmptNbkNPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oversampling and Undersampling Methods"
      ],
      "metadata": {
        "id": "7sVGxYp9N6If"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWrC7QkfX-ct"
      },
      "source": [
        "## Over Sampling with SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.combine import SMOTEENN,SMOTETomek\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.pipeline import Pipeline as pp\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "HFo02bWZezCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model():\n",
        "  models = []\n",
        "  models.append(small_model)\n",
        "  models.append(improved_model)\n",
        "  models.append(over_model)\n",
        "  return models"
      ],
      "metadata": {
        "id": "Bbq_wfjvf1Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = base_model()\n",
        "models"
      ],
      "metadata": {
        "id": "S1VdLlxHf1b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['precision', 'recall', 'f1', 'average Precision-Recall (AUPRC)']"
      ],
      "metadata": {
        "id": "rDAXLLmwf1ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {model.__class__.__name__: {metric: [] for metric in metrics} for model in models}"
      ],
      "metadata": {
        "id": "c5azZmsCf1hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the models with SMOTE valuate the models on test data"
      ],
      "metadata": {
        "id": "6vpIjKxdf1kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,average_precision_score\n",
        "# loop through each model\n",
        "for model in models:\n",
        "    # fit the model without SMOTE\n",
        "    model.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))\n",
        "    \n",
        "    # fit the model with SMOTE\n",
        "    pipe = pp([('smote', SMOTE(random_state=0)), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))"
      ],
      "metadata": {
        "id": "dzvyGyzjf1nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the evaluation results\n",
        "print(\"Evaluation results:\")\n",
        "print(\"\\n\")\n",
        "for model_name, model_results in results.items():\n",
        "    print(\"-------------------\")\n",
        "    print(\"Model:\", model_name)\n",
        "    print(\"-------------------\")\n",
        "    \n",
        "    for metric, scores in model_results.items():\n",
        "    \n",
        "        print( metric,\": \" \"without SMOTE:\", scores[0], \"|| \" \"with SMOTE:\", scores[1])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "Kg8c2bbqf1qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaulate Oversampling with Adasyn"
      ],
      "metadata": {
        "id": "eb2CXS60HQgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# list of metrics to be evaluated\n",
        "metrics = ['precision', 'recall', 'f1', 'average Precision-Recall (AUPRC)']\n",
        "\n",
        "# dictionary to store evaluation results\n",
        "results = {model.__class__.__name__: {metric: [] for metric in metrics} for model in models}\n",
        "\n",
        "# loop through each model\n",
        "for model in models:\n",
        "    # fit the model without ADASYN\n",
        "    model.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))\n",
        "    \n",
        "    # fit the model with ADASYN\n",
        "    pipe = pp([('ADASYN', ADASYN(random_state=0)), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))"
      ],
      "metadata": {
        "id": "IAVidSlpf1sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaYkYfvayfcl"
      },
      "outputs": [],
      "source": [
        "# print the evaluation results\n",
        "print(\"Evaluation results:\")\n",
        "print(\"\\n\")\n",
        "for model_name, model_results in results.items():\n",
        "    print(\"-------------------\")\n",
        "    print(\"Model:\", model_name)\n",
        "    print(\"-------------------\")\n",
        "    \n",
        "    for metric, scores in model_results.items():\n",
        "    \n",
        "        print( metric,\": \" \"without ADASYN:\", scores[0], \"|| \" \"with ADASYN:\", scores[1])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Undersampling Evaulation with ENN(Editet Nearest Neighbours)"
      ],
      "metadata": {
        "id": "mz0WLl1IJPPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfm_n5BQyfmc"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "\n",
        "# list of metrics to be evaluated\n",
        "metrics = ['precision', 'recall', 'f1', 'average Precision-Recall (AUPRC)']\n",
        "\n",
        "# dictionary to store evaluation results\n",
        "results = {model.__class__.__name__: {metric: [] for metric in metrics} for model in models}\n",
        "\n",
        "# loop through each model\n",
        "for model in models:\n",
        "    # fit the model without EditedNearestNeighbours\n",
        "    model.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))\n",
        "    \n",
        "    # fit the model with EditedNearestNeighbours\n",
        "    pipe = pp([('EditedNearestNeighbours', EditedNearestNeighbours()), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh5O8o3JyfqU"
      },
      "outputs": [],
      "source": [
        "# print the evaluation results\n",
        "print(\"Evaluation results:\")\n",
        "print(\"\\n\")\n",
        "for model_name, model_results in results.items():\n",
        "    print(\"-------------------\")\n",
        "    print(\"Model:\", model_name)\n",
        "    print(\"-------------------\")\n",
        "    \n",
        "    for metric, scores in model_results.items():\n",
        "    \n",
        "        print( metric,\": \" \"without ENN:\", scores[0], \"|| \" \"with ENN:\", scores[1])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Combined Methods with SMOTEENN"
      ],
      "metadata": {
        "id": "qQ29hCCZN7Q7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cIJ8eBzyft-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b73872-3cb3-4856-cca8-49078d76fc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6207/6207 [==============================] - 31s 5ms/step - loss: 0.0011 - accuracy: 0.9997\n",
            "2660/2660 [==============================] - 5s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "\n",
        "# list of metrics to be evaluated\n",
        "metrics = ['precision', 'recall', 'f1', 'average Precision-Recall (AUPRC)']\n",
        "\n",
        "# dictionary to store evaluation results\n",
        "results = {model.__class__.__name__: {metric: [] for metric in metrics} for model in models}\n",
        "\n",
        "# loop through each model\n",
        "for model in models:\n",
        "    # fit the model without SMOTEENN\n",
        "    model.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))\n",
        "    \n",
        "    # fit the model with SMOTEENN\n",
        "    pipe = pp([('SMOTEENN', SMOTEENN()), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vza1QeWky5Ii"
      },
      "outputs": [],
      "source": [
        "# print the evaluation results\n",
        "print(\"Evaluation results:\")\n",
        "print(\"\\n\")\n",
        "for model_name, model_results in results.items():\n",
        "    print(\"-------------------\")\n",
        "    print(\"Model:\", model_name)\n",
        "    print(\"-------------------\")\n",
        "    \n",
        "    for metric, scores in model_results.items():\n",
        "    \n",
        "        print( metric,\": \" \"without SMOTEENN:\", scores[0], \"|| \" \"with SMOTEENN:\", scores[1])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Combined Methods with SMOTETomek"
      ],
      "metadata": {
        "id": "58qxX3I3OhKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY1zGNdky5L4"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# list of metrics to be evaluated\n",
        "metrics = ['precision', 'recall', 'f1', 'average Precision-Recall (AUPRC)']\n",
        "\n",
        "# dictionary to store evaluation results\n",
        "results = {model.__class__.__name__: {metric: [] for metric in metrics} for model in models}\n",
        "\n",
        "# loop through each model\n",
        "for model in models:\n",
        "    # fit the model without SMOTEENN\n",
        "    model.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))\n",
        "    \n",
        "    # fit the model with SMOTEENN\n",
        "    pipe = pp([('SMOTETomek', SMOTETomek()), ('model', model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    # evaluate the model on test data\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    results[model.__class__.__name__]['precision'].append(precision_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['recall'].append(recall_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['f1'].append(f1_score(y_test, np.round(y_pred)))\n",
        "    results[model.__class__.__name__]['average Precision-Recall (AUPRC)'].append(average_precision_score(y_test, np.round(y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ71m4pay5Ow"
      },
      "outputs": [],
      "source": [
        "# print the evaluation results\n",
        "print(\"Evaluation results:\")\n",
        "print(\"\\n\")\n",
        "for model_name, model_results in results.items():\n",
        "    print(\"-------------------\")\n",
        "    print(\"Model:\", model_name)\n",
        "    print(\"-------------------\")\n",
        "    \n",
        "    for metric, scores in model_results.items():\n",
        "    \n",
        "        print( metric,\": \" \"without SMOTETomek:\", scores[0], \"|| \" \"with SMOTETomek:\", scores[1])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Modelling with Stradified-k fold"
      ],
      "metadata": {
        "id": "XfgGRySmAQDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4mt8JuuoKl5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratio = sum(df['Class'])/len(df)"
      ],
      "metadata": {
        "id": "4m-_QDEYAjf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class ratio:{ratio}\")"
      ],
      "metadata": {
        "id": "1dYre85tAgLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from imblearn.under_sampling import NearMiss\n",
        "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import Counter\n",
        "\n",
        "undersample_X = df.drop('Class', axis=1)\n",
        "undersample_y = df['Class']\n",
        "\n",
        "sss = StratifiedGroupKFold(n_splits=5,shuffle=True)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
        "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
        "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
        "    \n",
        "undersample_Xtrain = undersample_Xtrain.values\n",
        "undersample_Xtest = undersample_Xtest.values\n",
        "undersample_ytrain = undersample_ytrain.values\n",
        "undersample_ytest = undersample_ytest.values \n",
        "\n",
        "undersample_accuracy = []\n",
        "undersample_precision = []\n",
        "undersample_recall = []\n",
        "undersample_f1 = []\n",
        "undersample_auc = []\n",
        "\n",
        "# Implementing NearMiss Technique \n",
        "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
        "X_nearmiss, y_nearmiss = NearMiss().fit_resample(undersample_X.values, undersample_y.values)\n",
        "print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
        "# Cross Validating the right way\n",
        "\n",
        "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
        "    undersample_pipeline = pp(NearMiss(,improved_model) # SMOTE happens during Cross Validation not before..\n",
        "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
        "    undersample_prediction = undersample_pipeline.predict(undersample_Xtrain[test])\n",
        "    \n",
        "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
        "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
      ],
      "metadata": {
        "id": "oLLNz9xhAjrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrXtHZjsuuZE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "labels = ['No Fraud', 'Fraud']\n",
        "\n",
        "fig = plt.figure(figsize=(16,8))\n",
        "\n",
        "small_model_matrix = confusion_matrix(y_test,np.round(predictions))\n",
        "improved_model_matrix = confusion_matrix(y_test,np.round(improved_model_predictions))\n",
        "over_model_matrix = confusion_matrix(y_test,np.round(over_model_predictions))\n",
        "\n",
        "\n",
        "#small_model\n",
        "fig.add_subplot(221)\n",
        "sns.heatmap(confusion_matrix(y_test,np.round(predictions)),annot=True)\n",
        "\n",
        "#improved_model\n",
        "fig.add_subplot(222)\n",
        "sns.heatmap(improved_model_predictions,annot=True)\n",
        "\n",
        "#after oversampling\n",
        "fig.add_subplot(223)\n",
        "sns.heatmap(over_model_matrix,annot=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "id": "5ruOANrzETLI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZLPmQiFEczO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1EtdODL96dkRLqARR4MrZZezfhnaPs4O6",
      "authorship_tag": "ABX9TyNAP1FFZFO0i7UmiufafM+A",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}